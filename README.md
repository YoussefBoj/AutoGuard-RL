# ğŸš— AutoGuard-RL

**Vision-Language Guided Safe Reinforcement Learning for Autonomous Driving**

A research-oriented autonomous driving system that combines Vision-Language Models (CLIP/BLIP), World Models (Dreamer-style RSSM), and Safe Reinforcement Learning to create safety-aware self-driving agents.

---

## ğŸŒŸ Features

- **ğŸ”’ Safety-First Learning**: Uses CLIP to assess scene safety and guide policy updates
- **ğŸŒ World Model Imagination**: Predicts future states without environment interaction
- **ğŸ¯ Safe RL Agent**: Actor-Critic with VL-SAFE algorithm for balanced reward-safety optimization
- **ğŸ® Easy Testing**: Includes dummy environment for quick prototyping
- **ğŸ“Š Real-time Monitoring**: TensorBoard integration for training visualization
- **ğŸ”§ Production Ready**: Modular, well-documented, and tested components

---

## ğŸ—ï¸ Architecture

```
Camera Input (CARLA/BDD100K)
        â†“
  CLIP Encoder â†’ Safety Risk Score (0-1)
        â†“
  World Model (RSSM) â†’ Latent Dynamics + Imagination
        â†“
  Safe RL Agent â†’ Action [steering, throttle]
        â†“
  Environment â†’ Reward + Cost
        â†“
  Dashboard â†’ Visualization
```

**Key Innovation**: Safety-aware policy weighting using vision-language models:
```
w = p(safe) Ã— exp(Î²â‚ Ã— A_reward) + (1-p(safe)) Ã— exp(-Î²â‚‚ Ã— A_cost)
```

---

## ğŸš€ Quick Start

### Installation

```bash
# Clone repository
git clone https://github.com/yourusername/AutoGuard-RL.git
cd AutoGuard-RL

# Run setup script
bash setup.sh

# Install dependencies
pip install -r requirements.txt
```

### Test Components

```bash
python test_components.py
```

Expected output:
```
âœ“ CLIP Encoder working
âœ“ World Model working
âœ“ Safe Actor-Critic working
âœ“ Environment working
âœ“ Integration working
```

### Train Agent

```bash
# Quick test (10 episodes)
python train.py --episodes 10

# Full training (1000 episodes)
python train.py --episodes 1000

# Monitor with TensorBoard
tensorboard --logdir runs/
```

---

## ğŸ“ Project Structure

```
AutoGuard-RL/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ clip_encoder.py          # CLIP-based safety scorer
â”‚   â”œâ”€â”€ rssm_worldmodel.py       # Dreamer-style world model
â”‚   â””â”€â”€ actor_critic_safe.py     # Safe RL agent (VL-SAFE)
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ carla_env_wrapper.py     # Environment wrapper + replay buffer
â”‚   â”œâ”€â”€ reward_functions.py      # Reward shaping logic
â”‚   â””â”€â”€ safety_monitor.py        # Safety violation tracking
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ model_config.yaml        # Model hyperparameters
â”‚   â””â”€â”€ train_config.yaml        # Training configuration
â”œâ”€â”€ train.py                     # Main training script
â”œâ”€â”€ test_components.py           # Component testing
â””â”€â”€ requirements.txt             # Dependencies
```

---

## ğŸ”§ Configuration

Edit `config/train_config.yaml` to customize training:

```yaml
training:
  num_epochs: 100
  batch_size: 32
  learning_rate: 0.0003
  safety_lambda: 10.0    # Safety penalty weight

environment:
  use_carla: false        # Set true for CARLA simulator
  image_size: [84, 84]
  max_episode_steps: 333
```

---

## ğŸ“Š Results

After 100 episodes with DummyEnv:

| Metric | Initial | After Training |
|--------|---------|----------------|
| Episode Reward | ~15 | ~50 |
| Safety Cost | ~0.5 | ~0.1 |
| Episode Length | ~45 | ~80 |

---

## ğŸ§  Core Components

### 1. CLIP Safety Encoder

Computes semantic similarity between driving scenes and unsafe text prompts:

```python
from models.clip_encoder import ClipEncoder

encoder = ClipEncoder()
safety_score = encoder.safety_score(image)  # Returns 0.0 (safe) to 1.0 (unsafe)
```

### 2. RSSM World Model

Learns latent dynamics for imagination-based planning:

```python
from models.rssm_worldmodel import WorldModel

model = WorldModel(config)
output = model(images, actions)
# Returns: reconstructed images, predicted rewards/costs
```

### 3. Safe Actor-Critic Agent

Optimizes policy with safety-aware weighting:

```python
from models.actor_critic_safe import SafeActorCritic

agent = SafeActorCritic(config)
action = agent.select_action(state)  # Returns [steering, throttle]
```

---

## ğŸ“š Research Foundation

This project builds on recent advances in autonomous driving research:

1. **VL-SAFE** - Vision-Language Guided Safety-Aware Reinforcement Learning (arXiv 2025)
2. **DreamerV3** - Mastering Diverse Domains through World Models (Hafner et al., 2023)
3. **SafeDreamer** - Safe Reinforcement Learning with World Models (ICLR 2024)
4. **DriveDreamer4D** - World Models for Driving Scene Representation (CVPR 2025)

---

## ğŸ¯ Roadmap

- [x] Core components implementation
- [x] Dummy environment for testing
- [x] Training pipeline with TensorBoard
- [ ] Full CARLA integration
- [ ] BDD100K dataset preprocessing
- [ ] Multi-task learning
- [ ] Vision transformer backbone
- [ ] Streamlit dashboard

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request


---

## ğŸ™ Acknowledgments

- [CARLA Simulator](https://carla.org/) for autonomous driving simulation
- [OpenAI CLIP](https://github.com/openai/CLIP) for vision-language understanding
- [DreamerV3](https://github.com/danijar/dreamerv3) for world model architecture
- [BDD100K](https://bdd-data.berkeley.edu/) for driving dataset



---

**Built with â¤ï¸ for safer autonomous driving**
